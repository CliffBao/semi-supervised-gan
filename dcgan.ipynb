{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "nz = 100\n",
    "ngf = 32\n",
    "ndf = 64\n",
    "nc = 3\n",
    "batch_size = 64\n",
    "beta1 = 0.5\n",
    "image_size = 64\n",
    "lr = 0.0002\n",
    "epochs = 25\n",
    "# dataroot = \"./svhn\"\n",
    "dataroot = \"./cifar10\"\n",
    "workers = 1\n",
    "out_dir = \"./dcgan_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
     ]
    }
   ],
   "source": [
    "dataset = dset.CIFAR10(\n",
    "    root=dataroot,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netG, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(     nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs.data, torch.cuda.FloatTensor) and use_gpu:\n",
    "            output = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "        else:\n",
    "            output = self.main(inputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d (100, 256, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d (256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d (128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d (64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d (32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = _netG()\n",
    "netG.apply(weights_init)\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_netD, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs.data, torch.cuda.FloatTensor) and use_gpu:\n",
    "            output = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "        else:\n",
    "            output = self.main(inputs)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d (64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d (128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d (256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU(0.2, inplace)\n",
      "    (11): Conv2d (512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD = _netD()\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criterion, labels, inputs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "\n",
    "inputs = torch.FloatTensor(batch_size, 3, image_size, image_size)\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "label = torch.FloatTensor(batch_size)\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "if use_gpu:\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    inputs, label = inputs.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()\n",
    "\n",
    "fixed_noise = Variable(fixed_noise)\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][0/782] Loss_D: 1.9807 Loss_G: 6.7601 D(x): 0.6410 D(G(z)): 0.7007 / 0.0017\n",
      "[0/25][1/782] Loss_D: 1.0047 Loss_G: 4.3817 D(x): 0.4793 D(G(z)): 0.0653 / 0.0198\n",
      "[0/25][2/782] Loss_D: 0.4207 Loss_G: 4.1543 D(x): 0.8543 D(G(z)): 0.1974 / 0.0191\n",
      "[0/25][3/782] Loss_D: 0.5109 Loss_G: 5.1706 D(x): 0.8762 D(G(z)): 0.2661 / 0.0082\n",
      "[0/25][4/782] Loss_D: 0.3649 Loss_G: 5.9571 D(x): 0.8978 D(G(z)): 0.1970 / 0.0034\n",
      "[0/25][5/782] Loss_D: 0.4023 Loss_G: 5.0804 D(x): 0.8228 D(G(z)): 0.0954 / 0.0086\n",
      "[0/25][6/782] Loss_D: 0.4272 Loss_G: 6.4189 D(x): 0.9355 D(G(z)): 0.2682 / 0.0023\n",
      "[0/25][7/782] Loss_D: 0.3424 Loss_G: 6.1785 D(x): 0.8636 D(G(z)): 0.1243 / 0.0029\n",
      "[0/25][8/782] Loss_D: 0.3768 Loss_G: 6.0307 D(x): 0.8447 D(G(z)): 0.1485 / 0.0032\n",
      "[0/25][9/782] Loss_D: 0.3379 Loss_G: 6.7913 D(x): 0.8889 D(G(z)): 0.1788 / 0.0014\n",
      "[0/25][10/782] Loss_D: 0.2853 Loss_G: 6.6505 D(x): 0.8863 D(G(z)): 0.1096 / 0.0018\n",
      "[0/25][11/782] Loss_D: 0.3104 Loss_G: 7.3809 D(x): 0.9113 D(G(z)): 0.1512 / 0.0009\n",
      "[0/25][12/782] Loss_D: 0.3875 Loss_G: 6.0752 D(x): 0.8230 D(G(z)): 0.0789 / 0.0034\n",
      "[0/25][13/782] Loss_D: 0.3440 Loss_G: 8.3239 D(x): 0.9226 D(G(z)): 0.2047 / 0.0003\n",
      "[0/25][14/782] Loss_D: 0.1943 Loss_G: 7.3801 D(x): 0.8990 D(G(z)): 0.0613 / 0.0009\n",
      "[0/25][15/782] Loss_D: 0.2121 Loss_G: 7.3783 D(x): 0.9273 D(G(z)): 0.1089 / 0.0008\n",
      "[0/25][16/782] Loss_D: 0.2355 Loss_G: 8.2115 D(x): 0.9169 D(G(z)): 0.1207 / 0.0004\n",
      "[0/25][17/782] Loss_D: 0.1174 Loss_G: 7.2667 D(x): 0.9305 D(G(z)): 0.0402 / 0.0012\n",
      "[0/25][18/782] Loss_D: 0.2945 Loss_G: 9.2608 D(x): 0.9287 D(G(z)): 0.1676 / 0.0002\n",
      "[0/25][19/782] Loss_D: 0.1371 Loss_G: 7.5552 D(x): 0.9010 D(G(z)): 0.0226 / 0.0009\n",
      "[0/25][20/782] Loss_D: 0.2030 Loss_G: 8.6521 D(x): 0.9567 D(G(z)): 0.1355 / 0.0002\n",
      "[0/25][21/782] Loss_D: 0.1053 Loss_G: 7.9065 D(x): 0.9445 D(G(z)): 0.0399 / 0.0006\n",
      "[0/25][22/782] Loss_D: 0.1608 Loss_G: 7.6795 D(x): 0.9290 D(G(z)): 0.0738 / 0.0006\n",
      "[0/25][23/782] Loss_D: 0.1899 Loss_G: 9.9429 D(x): 0.9520 D(G(z)): 0.1178 / 0.0001\n",
      "[0/25][24/782] Loss_D: 0.2081 Loss_G: 7.6318 D(x): 0.8629 D(G(z)): 0.0159 / 0.0008\n",
      "[0/25][25/782] Loss_D: 0.1479 Loss_G: 9.2986 D(x): 0.9711 D(G(z)): 0.1012 / 0.0001\n",
      "[0/25][26/782] Loss_D: 0.0858 Loss_G: 8.1951 D(x): 0.9482 D(G(z)): 0.0248 / 0.0004\n",
      "[0/25][27/782] Loss_D: 0.0984 Loss_G: 8.7287 D(x): 0.9713 D(G(z)): 0.0634 / 0.0002\n",
      "[0/25][28/782] Loss_D: 0.0717 Loss_G: 9.5425 D(x): 0.9891 D(G(z)): 0.0560 / 0.0001\n",
      "[0/25][29/782] Loss_D: 0.0545 Loss_G: 8.4623 D(x): 0.9728 D(G(z)): 0.0229 / 0.0003\n",
      "[0/25][30/782] Loss_D: 0.0919 Loss_G: 9.2765 D(x): 0.9732 D(G(z)): 0.0593 / 0.0001\n",
      "[0/25][31/782] Loss_D: 0.1287 Loss_G: 8.5904 D(x): 0.9202 D(G(z)): 0.0358 / 0.0003\n",
      "[0/25][32/782] Loss_D: 0.0908 Loss_G: 9.6662 D(x): 0.9726 D(G(z)): 0.0583 / 0.0001\n",
      "[0/25][33/782] Loss_D: 0.0385 Loss_G: 8.6495 D(x): 0.9791 D(G(z)): 0.0167 / 0.0003\n",
      "[0/25][34/782] Loss_D: 0.0838 Loss_G: 9.7335 D(x): 0.9773 D(G(z)): 0.0570 / 0.0001\n",
      "[0/25][35/782] Loss_D: 0.0390 Loss_G: 9.1800 D(x): 0.9864 D(G(z)): 0.0240 / 0.0002\n",
      "[0/25][36/782] Loss_D: 0.0565 Loss_G: 8.9766 D(x): 0.9764 D(G(z)): 0.0306 / 0.0002\n",
      "[0/25][37/782] Loss_D: 0.0906 Loss_G: 9.6032 D(x): 0.9583 D(G(z)): 0.0440 / 0.0001\n",
      "[0/25][38/782] Loss_D: 0.0835 Loss_G: 9.4935 D(x): 0.9593 D(G(z)): 0.0324 / 0.0001\n",
      "[0/25][39/782] Loss_D: 0.0493 Loss_G: 9.2648 D(x): 0.9786 D(G(z)): 0.0262 / 0.0002\n",
      "[0/25][40/782] Loss_D: 0.0470 Loss_G: 9.9803 D(x): 0.9912 D(G(z)): 0.0365 / 0.0001\n",
      "[0/25][41/782] Loss_D: 0.0230 Loss_G: 8.9061 D(x): 0.9872 D(G(z)): 0.0098 / 0.0002\n",
      "[0/25][42/782] Loss_D: 0.0681 Loss_G: 10.1624 D(x): 0.9778 D(G(z)): 0.0431 / 0.0001\n",
      "[0/25][43/782] Loss_D: 0.0546 Loss_G: 8.7916 D(x): 0.9651 D(G(z)): 0.0101 / 0.0002\n",
      "[0/25][44/782] Loss_D: 0.0652 Loss_G: 11.0906 D(x): 0.9873 D(G(z)): 0.0497 / 0.0000\n",
      "[0/25][45/782] Loss_D: 0.0390 Loss_G: 9.6272 D(x): 0.9687 D(G(z)): 0.0051 / 0.0001\n",
      "[0/25][46/782] Loss_D: 0.0471 Loss_G: 10.1337 D(x): 0.9880 D(G(z)): 0.0336 / 0.0001\n",
      "[0/25][47/782] Loss_D: 0.0212 Loss_G: 9.5532 D(x): 0.9916 D(G(z)): 0.0126 / 0.0001\n",
      "[0/25][48/782] Loss_D: 0.0496 Loss_G: 9.5974 D(x): 0.9748 D(G(z)): 0.0221 / 0.0001\n",
      "[0/25][49/782] Loss_D: 0.0213 Loss_G: 9.4970 D(x): 0.9937 D(G(z)): 0.0148 / 0.0001\n",
      "[0/25][50/782] Loss_D: 0.0427 Loss_G: 10.0047 D(x): 0.9819 D(G(z)): 0.0206 / 0.0001\n",
      "[0/25][51/782] Loss_D: 0.0325 Loss_G: 8.9140 D(x): 0.9763 D(G(z)): 0.0074 / 0.0002\n",
      "[0/25][52/782] Loss_D: 0.0514 Loss_G: 11.3815 D(x): 0.9819 D(G(z)): 0.0317 / 0.0000\n",
      "[0/25][53/782] Loss_D: 0.0114 Loss_G: 10.4064 D(x): 0.9907 D(G(z)): 0.0019 / 0.0000\n",
      "[0/25][54/782] Loss_D: 0.0101 Loss_G: 8.4185 D(x): 0.9963 D(G(z)): 0.0063 / 0.0003\n",
      "[0/25][55/782] Loss_D: 0.0429 Loss_G: 11.6139 D(x): 0.9906 D(G(z)): 0.0323 / 0.0000\n",
      "[0/25][56/782] Loss_D: 0.0099 Loss_G: 10.7345 D(x): 0.9920 D(G(z)): 0.0018 / 0.0000\n",
      "[0/25][57/782] Loss_D: 0.0218 Loss_G: 8.2388 D(x): 0.9850 D(G(z)): 0.0050 / 0.0004\n",
      "[0/25][58/782] Loss_D: 0.0739 Loss_G: 17.7465 D(x): 0.9957 D(G(z)): 0.0656 / 0.0000\n",
      "[0/25][59/782] Loss_D: 0.0147 Loss_G: 18.9328 D(x): 0.9856 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][60/782] Loss_D: 0.0128 Loss_G: 16.6018 D(x): 0.9876 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][61/782] Loss_D: 0.0376 Loss_G: 11.8958 D(x): 0.9662 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][62/782] Loss_D: 0.0231 Loss_G: 6.4143 D(x): 0.9819 D(G(z)): 0.0034 / 0.0022\n",
      "[0/25][63/782] Loss_D: 0.3647 Loss_G: 27.6273 D(x): 0.9985 D(G(z)): 0.2941 / 0.0000\n",
      "[0/25][64/782] Loss_D: 0.9004 Loss_G: 27.6307 D(x): 0.6342 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][65/782] Loss_D: 0.2771 Loss_G: 27.6309 D(x): 0.8472 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][66/782] Loss_D: 0.0015 Loss_G: 27.6309 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][67/782] Loss_D: 0.0015 Loss_G: 27.6309 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][68/782] Loss_D: 0.0020 Loss_G: 27.6309 D(x): 0.9980 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][69/782] Loss_D: 0.0010 Loss_G: 27.6309 D(x): 0.9990 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][70/782] Loss_D: 0.0006 Loss_G: 27.6309 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][71/782] Loss_D: 0.0006 Loss_G: 27.6309 D(x): 0.9994 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][72/782] Loss_D: 0.0015 Loss_G: 27.6309 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][73/782] Loss_D: 0.0007 Loss_G: 27.6309 D(x): 0.9993 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][74/782] Loss_D: 0.0009 Loss_G: 27.6308 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][75/782] Loss_D: 0.0009 Loss_G: 27.6309 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][76/782] Loss_D: 0.0015 Loss_G: 27.6309 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][77/782] Loss_D: 0.0009 Loss_G: 27.6309 D(x): 0.9991 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][78/782] Loss_D: 0.0017 Loss_G: 27.6308 D(x): 0.9984 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][79/782] Loss_D: 0.0005 Loss_G: 27.6309 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][80/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][81/782] Loss_D: 0.0005 Loss_G: 27.6309 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][82/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][83/782] Loss_D: 0.0008 Loss_G: 27.6309 D(x): 0.9992 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][84/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][85/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][86/782] Loss_D: 0.0004 Loss_G: 27.6308 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][87/782] Loss_D: 0.0016 Loss_G: 27.6309 D(x): 0.9985 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][88/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][89/782] Loss_D: 0.0010 Loss_G: 27.6309 D(x): 0.9990 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][90/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][91/782] Loss_D: 0.0003 Loss_G: 27.6308 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][92/782] Loss_D: 0.0002 Loss_G: 27.6308 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][93/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][94/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][95/782] Loss_D: 0.0005 Loss_G: 27.6309 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][96/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][97/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][98/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/25][99/782] Loss_D: 0.0001 Loss_G: 27.6306 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][100/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][101/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][102/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][103/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][104/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][105/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][106/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][107/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][108/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][109/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][110/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][111/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][112/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][113/782] Loss_D: 0.0005 Loss_G: 27.6309 D(x): 0.9995 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][114/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][115/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][116/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][117/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][118/782] Loss_D: 0.0003 Loss_G: 27.6308 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][119/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][120/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][121/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][122/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][123/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][124/782] Loss_D: 0.0002 Loss_G: 27.6308 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][125/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][126/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][127/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][128/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][129/782] Loss_D: 0.0004 Loss_G: 27.6309 D(x): 0.9996 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][130/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][131/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][132/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][133/782] Loss_D: 0.0002 Loss_G: 27.6308 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][134/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][135/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][136/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][137/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][138/782] Loss_D: 0.0002 Loss_G: 27.6308 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][139/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][140/782] Loss_D: 0.0002 Loss_G: 27.6308 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][141/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][142/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][143/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][144/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][145/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][146/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][147/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][148/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][149/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][150/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][151/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][152/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][153/782] Loss_D: 0.0000 Loss_G: 27.6309 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][154/782] Loss_D: 0.0002 Loss_G: 27.6308 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][155/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][156/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][157/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][158/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][159/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][160/782] Loss_D: 0.0003 Loss_G: 27.6309 D(x): 0.9997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][161/782] Loss_D: 0.0001 Loss_G: 27.6308 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][162/782] Loss_D: 0.0001 Loss_G: 27.6308 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][163/782] Loss_D: 0.0001 Loss_G: 27.6308 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][164/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][165/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][166/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][167/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][168/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][169/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][170/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][171/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][172/782] Loss_D: 0.0002 Loss_G: 27.6309 D(x): 0.9998 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][173/782] Loss_D: 0.0000 Loss_G: 27.6309 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][174/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][175/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][176/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][177/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][178/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][179/782] Loss_D: 0.0001 Loss_G: 27.6308 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][180/782] Loss_D: 0.0000 Loss_G: 27.6309 D(x): 1.0000 D(G(z)): 0.0000 / 0.0000\n",
      "[0/25][181/782] Loss_D: 0.0001 Loss_G: 27.6309 D(x): 0.9999 D(G(z)): 0.0000 / 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/d068545/anaconda3/envs/aind2/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c841875bfaeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlabelv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0merrD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0merrD_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-bfc678619db5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 277\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aind2/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, _ = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        if use_gpu:\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        inputs.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        label.resize_(batch_size).fill_(real_label)\n",
    "        inputv = Variable(inputs)\n",
    "        labelv = Variable(label)\n",
    "\n",
    "        output = netD(inputv)\n",
    "        errD_real = criterion(output, labelv)\n",
    "        errD_real.backward()\n",
    "        D_x = output.data.mean()\n",
    "\n",
    "        # train with fake\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, labelv)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        labelv = Variable(label.fill_(real_label))  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, labelv)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.data.mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, epochs, i, len(dataloader),\n",
    "                 errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % out_dir,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (out_dir, epoch),\n",
    "                    normalize=True)\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (out_dir, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (out_dir, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
