{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvhnDataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "        self.svhn_dataset = self._create_dataset(image_size, split)\n",
    "        self.label_mask = self._create_label_mask()\n",
    "\n",
    "    def _create_dataset(self, image_size, split):\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5])\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "        return datasets.SVHN(root='./svhn', download=True, transform=transform, split=split)\n",
    "\n",
    "    def _is_train_dataset(self):\n",
    "        return True if self.split == 'train' else False\n",
    "\n",
    "    def _create_label_mask(self):\n",
    "        if self._is_train_dataset():\n",
    "            label_mask = torch.zeros(len(self.svhn_dataset)).float()\n",
    "            label_mask[0:1000] = 1\n",
    "            if self.use_gpu:\n",
    "                label_mask = label_mask.cuda()\n",
    "            return label_mask\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.svhn_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.svhn_dataset.__getitem__(idx)\n",
    "        if self._is_train_dataset():\n",
    "            return data, label, self.label_mask[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 1\n",
    "\n",
    "    svhn_train = SvhnDataset(image_size=image_size, split='train')\n",
    "    svhn_test = SvhnDataset(image_size=image_size, split='test')\n",
    "\n",
    "    svhn_loader_train = DataLoader(\n",
    "        dataset=svhn_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    svhn_loader_test = DataLoader(\n",
    "        dataset=svhn_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return svhn_loader_train, svhn_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv, deconv helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    '''\n",
    "    GAN generator\n",
    "    '''\n",
    "    def __init__(self, num_noise_channels, size_mult, lrelu_alpha, num_output_channels):\n",
    "        super(_netG, self).__init__()\n",
    "        self.lrelu_alpha = lrelu_alpha\n",
    "\n",
    "        # noise is going into a convolution\n",
    "        self.deconv1 = deconv(\n",
    "            c_in=num_noise_channels,\n",
    "            c_out=size_mult * 4,\n",
    "            k_size=4,\n",
    "            stride=1,\n",
    "            pad=0)\n",
    "        # (size_mult * 4) x 4 x 4\n",
    "\n",
    "        self.deconv2 = deconv(\n",
    "            c_in=size_mult * 4,\n",
    "            c_out=size_mult * 2,\n",
    "            k_size=4)\n",
    "        # (size_mult * 2) x 8 x 8\n",
    "\n",
    "        self.deconv3 = deconv(\n",
    "            c_in=size_mult * 2,\n",
    "            c_out=size_mult * 1,\n",
    "            k_size=4)\n",
    "        # (size_mult) x 16 x 16\n",
    "\n",
    "        self.deconv4 = deconv(\n",
    "            c_in=size_mult,\n",
    "            c_out=num_output_channels,\n",
    "            k_size=4,\n",
    "            bn=False)\n",
    "        # (num_output_channels) x 16 x 16\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = F.leaky_relu(self.deconv1(inputs), self.lrelu_alpha)\n",
    "        out = F.leaky_relu(self.deconv2(out), self.lrelu_alpha)\n",
    "        out = F.leaky_relu(self.deconv3(out), self.lrelu_alpha)\n",
    "        out = F.tanh(self.deconv4(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    '''\n",
    "    GAN discruminator\n",
    "    '''\n",
    "    def __init__(self, size_mult, lrelu_alpha, number_channels, drop_rate, num_classes):\n",
    "        super(_netD, self).__init__()\n",
    "        self.drop_rate = drop_rate\n",
    "        self.lrelu_alpha = lrelu_alpha\n",
    "        self.size_mult = size_mult\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # input is (number_channels) x 32 x 32\n",
    "        self.conv1 = conv(\n",
    "            c_in=number_channels,\n",
    "            c_out=size_mult,\n",
    "            k_size=3,\n",
    "            bn=False\n",
    "        )\n",
    "        # (size_mult) x 16 x 16\n",
    "\n",
    "        self.conv2 = conv(\n",
    "            c_in=size_mult,\n",
    "            c_out=size_mult,\n",
    "            k_size=3,\n",
    "        )\n",
    "        # (size_mult) x 8 x 8\n",
    "\n",
    "        self.conv3 = conv(\n",
    "            c_in=size_mult,\n",
    "            c_out=size_mult,\n",
    "            k_size=3,\n",
    "        )\n",
    "        # (size_mult) x 4 x 4\n",
    "\n",
    "        self.conv4 = conv(\n",
    "            c_in=size_mult,\n",
    "            c_out=size_mult * 2,\n",
    "            k_size=3,\n",
    "            stride=1\n",
    "        )\n",
    "        # (size_mult * 2) x 4 x 4\n",
    "\n",
    "        self.conv5 = conv(\n",
    "            c_in=size_mult * 2,\n",
    "            c_out=size_mult * 2,\n",
    "            k_size=3,\n",
    "            stride=1\n",
    "        )\n",
    "        # (size_mult * 2) x 4 x 4\n",
    "\n",
    "        self.conv6 = conv(\n",
    "            c_in=size_mult * 2,\n",
    "            c_out=size_mult * 2,\n",
    "            k_size=3,\n",
    "            stride=1,\n",
    "            pad=0,\n",
    "            bn=False\n",
    "        )\n",
    "        # (size_mult * 2) x 2 x 2\n",
    "\n",
    "        self.features = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        self.class_logits = nn.Linear(\n",
    "            in_features=(size_mult * 2) * 1 * 1,\n",
    "            out_features=num_classes)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        out = F.dropout2d(inputs, p=self.drop_rate/2.5)\n",
    "\n",
    "        out = F.leaky_relu(self.conv1(out), self.lrelu_alpha)\n",
    "        out = F.dropout2d(out, p=self.drop_rate)\n",
    "\n",
    "        out = F.leaky_relu(self.conv2(out), self.lrelu_alpha)\n",
    "\n",
    "        out = F.leaky_relu(self.conv3(out), self.lrelu_alpha)\n",
    "        out = F.dropout2d(out, p=self.drop_rate)\n",
    "\n",
    "        out = F.leaky_relu(self.conv4(out), self.lrelu_alpha)\n",
    "\n",
    "        out = F.leaky_relu(self.conv5(out), self.lrelu_alpha)\n",
    "\n",
    "        out = F.leaky_relu(self.conv6(out), self.lrelu_alpha)\n",
    "\n",
    "        features = self.features(out)\n",
    "        features = features.squeeze()\n",
    "\n",
    "        class_logits = self.class_logits(features)\n",
    "\n",
    "        # calculate gan logits\n",
    "        max_val, _ = torch.max(class_logits, 1, keepdim=True)\n",
    "        stable_class_logits = class_logits - max_val\n",
    "        max_val = torch.squeeze(max_val)\n",
    "        gan_logits = torch.log(torch.sum(torch.exp(stable_class_logits), 1)) + max_val\n",
    "\n",
    "        out = F.softmax(class_logits, dim=0)\n",
    "\n",
    "        return out, class_logits, gan_logits, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import _netG, _netD\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, svhn_loader_train, svhn_loader_test, batch_size):\n",
    "        self.nz = 100\n",
    "        self.real_image_size = (3, 32, 32)\n",
    "        self.lrelu_alpha = 1e-2\n",
    "        self.drop_rate = .5\n",
    "        self.g_size_mult = 32\n",
    "        self.d_size_mult = 64\n",
    "        self.num_classes = 10\n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "        self.learning_rate = 3e-3\n",
    "        self.beta1 = .5\n",
    "        self.svhn_loader_train = svhn_loader_train\n",
    "        self.svhn_loader_test = svhn_loader_test\n",
    "        self.epochs = 25\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.generator, self.discriminator = self._build_model()\n",
    "        self.g_optimizer, self.d_optimizer = self._create_optimizers()\n",
    "\n",
    "    def _build_model(self):\n",
    "        generator = _netG(\n",
    "            self.nz, self.g_size_mult, self.lrelu_alpha,\n",
    "            self.real_image_size[0])\n",
    "        generator.apply(self._weights_init)\n",
    "        # TODO: load weights from file if it exists\n",
    "\n",
    "        discriminator = _netD(\n",
    "            self.d_size_mult, self.lrelu_alpha, self.real_image_size[0],\n",
    "            self.drop_rate, self.num_classes)\n",
    "        discriminator.apply(self._weights_init)\n",
    "        # TODO: load weights from file if it exists\n",
    "\n",
    "        if self.use_gpu:\n",
    "            generator = generator.cuda()\n",
    "            discriminator = discriminator.cuda()\n",
    "\n",
    "        return generator, discriminator\n",
    "\n",
    "    def _weights_init(self, module):\n",
    "        '''\n",
    "        Custom weights initialization called on generator and discriminator\n",
    "        '''\n",
    "        classname = module.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            module.weight.data.normal_(0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            module.weight.data.normal_(1.0, 0.02)\n",
    "            module.bias.data.fill_(0)\n",
    "\n",
    "    def _create_optimizers(self):\n",
    "        g_params = list(self.generator.parameters())\n",
    "        d_params = list(self.discriminator.parameters())\n",
    "\n",
    "        g_optimizer = optim.Adam(g_params, self.learning_rate)\n",
    "        d_optimizer = optim.Adam(d_params, self.learning_rate)\n",
    "\n",
    "        return g_optimizer, d_optimizer\n",
    "\n",
    "    def _to_var(self, x):\n",
    "        if self.use_gpu:\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "\n",
    "    def _one_hot(self, x):\n",
    "        ones = torch.sparse.torch.eye(self.num_classes)\n",
    "        one_hot = ones.index_select(0, x.data)\n",
    "        return Variable(one_hot)\n",
    "\n",
    "    def train(self):\n",
    "        svhn_iter = iter(self.svhn_loader_train)\n",
    "        iter_per_epoch = len(svhn_iter)\n",
    "        print(iter_per_epoch)\n",
    "\n",
    "        d_gan_criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        noise = torch.FloatTensor(self.batch_size, self.nz, 1, 1)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            masked_correct = 0\n",
    "            num_samples = 0\n",
    "            total_count_samples = 0\n",
    "            loop_count = 0\n",
    "\n",
    "            for _, data in enumerate(self.svhn_loader_train):\n",
    "                # load svhn dataset\n",
    "                svhn_data, svhn_labels, label_mask = data\n",
    "                svhn_data = self._to_var(svhn_data)\n",
    "                svhn_labels = self._to_var(svhn_labels).long().squeeze()\n",
    "                label_mask = self._to_var(label_mask).float().squeeze()\n",
    "\n",
    "                # -------------- train discriminator --------------\n",
    "\n",
    "                # train with real images\n",
    "                self.d_optimizer.zero_grad()\n",
    "\n",
    "                # d_out == softmax(d_class_logits)\n",
    "                d_out, d_class_logits_on_data, d_gan_logits_real, d_sample_features = self.discriminator(svhn_data)\n",
    "                d_gan_labels_real = self._to_var(torch.ones_like(d_gan_logits_real.data))\n",
    "                d_gan_loss_real = d_gan_criterion(\n",
    "                    d_gan_logits_real,\n",
    "                    d_gan_labels_real)\n",
    "\n",
    "                # train with fake images\n",
    "                noise.resize_(self.batch_size, self.nz, 1, 1).normal_(0, 1)\n",
    "                noise_var = self._to_var(noise)\n",
    "                fake = self.generator(noise_var)\n",
    "\n",
    "                # call detach() to avoid backprop for generator here\n",
    "                _, _, d_gan_logits_fake, _ = self.discriminator(fake.detach())\n",
    "\n",
    "                d_gan_labels_fake = self._to_var(torch.zeros_like(d_gan_logits_fake.data))\n",
    "                d_gan_loss_fake = d_gan_criterion(\n",
    "                    d_gan_logits_fake,\n",
    "                    d_gan_labels_fake)\n",
    "\n",
    "                d_gan_loss = d_gan_loss_real + d_gan_loss_fake\n",
    "\n",
    "                # d_out == softmax(d_class_logits)\n",
    "                # see https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits/39499486#39499486\n",
    "                svhn_labels_one_hot = self._one_hot(svhn_labels)\n",
    "                d_class_loss_entropy = -torch.sum(svhn_labels_one_hot * torch.log(d_out), dim=1)\n",
    "                \n",
    "                # d_class_loss_entropy = d_class_criterion(\n",
    "                #     d_class_logits_on_data,\n",
    "                #     self._one_hot(svhn_labels)\n",
    "                # )\n",
    "\n",
    "                d_class_loss_entropy = d_class_loss_entropy.squeeze()\n",
    "                delim = torch.max(torch.Tensor([1.0, torch.sum(label_mask.data)]))\n",
    "                d_class_loss = torch.sum(label_mask * d_class_loss_entropy) / delim\n",
    "                \n",
    "                d_loss = d_gan_loss + d_class_loss\n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # -------------- update generator --------------\n",
    "                \n",
    "                self.g_optimizer.zero_grad()\n",
    "\n",
    "                # call discriminator again to do backprop for generator here\n",
    "                _, _, _, d_data_features = self.discriminator(fake)\n",
    "                \n",
    "                # Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\n",
    "                # This loss consists of minimizing the absolute difference between the expected features\n",
    "                # on the data and the expected features on the generated samples.\n",
    "                # This loss works better for semi-supervised learning than the tradition GAN losses.\n",
    "                data_features_mean = torch.mean(d_data_features, dim=0)\n",
    "                sample_features_mean = torch.mean(d_sample_features.detach(), dim=0)\n",
    "                \n",
    "                g_loss = torch.mean(torch.abs(data_features_mean - sample_features_mean))\n",
    "\n",
    "                _, pred_class = torch.max(d_class_logits_on_data, 1)\n",
    "                eq = torch.eq(svhn_labels, pred_class)\n",
    "                correct = torch.sum(eq.float())\n",
    "                masked_correct += torch.sum(label_mask * eq.float())\n",
    "                num_samples += torch.sum(label_mask)\n",
    "\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                total_count_samples += len(svhn_labels)\n",
    "                loop_count += 1\n",
    "                if loop_count%10 == 0:\n",
    "                    print('Training:\\tepoch {}/{}\\tdiscr. gan loss {}\\tdiscr. class loss {}\\tgen loss {}\\tsamples {}/{}'.\n",
    "                            format(epoch, self.epochs, d_gan_loss.data[0], d_class_loss.data[0], g_loss.data[0], \n",
    "                                total_count_samples, len(self.svhn_loader_train)))\n",
    "                    \n",
    "            accuracy = masked_correct.data[0]/max(1.0, num_samples.data[0])\n",
    "            print('Training:\\tepoch {}/{}\\taccuracy {}'.format(epoch, self.epochs, accuracy))\n",
    "\n",
    "            total_count_samples = 0\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "            loop_count = 0\n",
    "            for _, data in enumerate(self.svhn_loader_test):\n",
    "                # load svhn dataset\n",
    "                svhn_data, svhn_labels = data\n",
    "                svhn_data = self._to_var(svhn_data)\n",
    "                svhn_labels = svhn_labels.long().squeeze()\n",
    "\n",
    "                # -------------- train discriminator --------------\n",
    "\n",
    "                # train with real images\n",
    "                d_out, _, _, _ = self.discriminator(svhn_data)\n",
    "                _, pred_idx = torch.max(d_out.data, 1)\n",
    "                eq = torch.eq(svhn_labels, pred_idx)\n",
    "                correct += torch.sum(eq.float())\n",
    "                num_samples += len(svhn_labels)\n",
    "                \n",
    "                total_count_samples += len(svhn_labels)\n",
    "                loop_count += 1\n",
    "                if loop_count%10 == 0:\n",
    "                    print('Test:\\tepoch {}/{}\\tsamples {}/{}'.format(\n",
    "                        epoch, self.epochs, total_count_samples, len(self.svhn_loader_test)))\n",
    "                \n",
    "            accuracy = correct/max(1.0, 1.0 * num_samples)\n",
    "            print('Test:\\tepoch {}/{}\\taccuracy {}'.format(epoch, self.epochs, accuracy))\n",
    "\n",
    "            # TODO: save checkpoints and the best model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: ./svhn/train_32x32.mat\n",
      "Using downloaded and verified file: ./svhn/test_32x32.mat\n"
     ]
    }
   ],
   "source": [
    "svhn_loader_train, svhn_loader_test = get_loader(image_size, batch_size)\n",
    "solver = Solver(svhn_loader_train, svhn_loader_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
