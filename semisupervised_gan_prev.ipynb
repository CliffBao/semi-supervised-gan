{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision.utils as vutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVHN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SvhnDataset(Dataset):\n",
    "    def __init__(self, image_size, split):\n",
    "        self.split = split\n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "\n",
    "        self.svhn_dataset = self._create_dataset(image_size, split)\n",
    "        self.label_mask = self._create_label_mask()\n",
    "\n",
    "    def _create_dataset(self, image_size, split):\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.5, 0.5, 0.5],\n",
    "            std=[0.5, 0.5, 0.5])\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            normalize])\n",
    "        return datasets.SVHN(root='./svhn', download=True, transform=transform, split=split)\n",
    "\n",
    "    def _is_train_dataset(self):\n",
    "        return True if self.split == 'train' else False\n",
    "\n",
    "    def _create_label_mask(self):\n",
    "        if self._is_train_dataset():\n",
    "            label_mask = torch.zeros(len(self.svhn_dataset)).float()\n",
    "            label_mask[0:1000] = 1\n",
    "            return label_mask\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.svhn_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data, label = self.svhn_dataset.__getitem__(idx)\n",
    "        if self._is_train_dataset():\n",
    "            return data, label, self.label_mask[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(image_size, batch_size):\n",
    "    num_workers = 1\n",
    "\n",
    "    svhn_train = SvhnDataset(image_size=image_size, split='train')\n",
    "    svhn_test = SvhnDataset(image_size=image_size, split='test')\n",
    "\n",
    "    svhn_loader_train = DataLoader(\n",
    "        dataset=svhn_train,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    svhn_loader_test = DataLoader(\n",
    "        dataset=svhn_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return svhn_loader_train, svhn_loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_loader_train, _ = get_loader(image_size=32, batch_size=36)\n",
    "image_iter = iter(svhn_loader_train)\n",
    "images, _, _ = image_iter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_images(images):\n",
    "    assert(len(images) >= 36)\n",
    "    fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5))\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "        img = images[idx].numpy()\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.imshow(img, aspect='equal')\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_images(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv, deconv helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    layers = []\n",
    "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv(c_in, c_out, k_size, stride=2, pad=1, bn=True):\n",
    "    layers = []\n",
    "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad, bias=False))\n",
    "    if bn:\n",
    "        layers.append(nn.BatchNorm2d(c_out))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ganLogits(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(_ganLogits, self).__init__()\n",
    "    \n",
    "    def forward(self, class_logits):\n",
    "        max_val, _ = torch.max(class_logits, 1, keepdim=True)\n",
    "        stable_class_logits = class_logits - max_val\n",
    "        max_val = torch.squeeze(max_val)\n",
    "        gan_logits = torch.log(torch.sum(torch.exp(stable_class_logits), 1)) + max_val\n",
    "        \n",
    "        return gan_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module):\n",
    "    '''\n",
    "    GAN generator\n",
    "    '''\n",
    "    def __init__(self, nz, ngf, alpha, nc, use_gpu):\n",
    "        super(_netG, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # noise is going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ngf * 4) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ngf * 2) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ngf) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # (nc) x 32 x 32\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs.data, torch.cuda.FloatTensor) and self.use_gpu:\n",
    "            out = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "        else:\n",
    "            out = self.main(inputs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    '''\n",
    "    GAN discruminator\n",
    "    '''\n",
    "    def __init__(self, ndf, alpha, nc, drop_rate, num_classes, use_gpu):\n",
    "        super(_netD, self).__init__()\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.Dropout2d(drop_rate/2.5),\n",
    "            \n",
    "            # input is (number_channels) x 32 x 32\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "            # (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf) x 8 x 8\n",
    "            nn.Conv2d(ndf, ndf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            nn.Dropout2d(drop_rate),\n",
    "            # (ndf) x 4 x 4\n",
    "            nn.Conv2d(ndf, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 4 x 4\n",
    "            nn.Conv2d(ndf * 2, ndf * 2, 3, 1, 0, bias=False),\n",
    "            nn.LeakyReLU(alpha),\n",
    "            # (ndf * 2) x 2 x 2\n",
    "        )\n",
    "        \n",
    "        self.features = nn.AvgPool2d(kernel_size=2)\n",
    "\n",
    "        self.class_logits = nn.Linear(\n",
    "            in_features=(ndf * 2) * 1 * 1,\n",
    "            out_features=num_classes)\n",
    "        \n",
    "        self.gan_logits = _ganLogits()\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if isinstance(inputs.data, torch.cuda.FloatTensor) and self.use_gpu:\n",
    "            out = nn.parallel.data_parallel(self.main, inputs, range(1))\n",
    "        else:\n",
    "            out = self.main(inputs)\n",
    "\n",
    "        features = self.features(out)\n",
    "        features = features.squeeze()\n",
    "\n",
    "        class_logits = self.class_logits(features)\n",
    "\n",
    "        gan_logits = self.gan_logits(class_logits)\n",
    "        \n",
    "        out = self.softmax(class_logits)\n",
    "\n",
    "        return out, class_logits, gan_logits, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, svhn_loader_train, svhn_loader_test, batch_size):\n",
    "        self.nz = 100\n",
    "        self.nc = 3\n",
    "        self.alpha = 0.2\n",
    "        self.drop_rate = .5\n",
    "        self.ngf = 32\n",
    "        self.ndf = 64\n",
    "        self.num_classes = 10\n",
    "        self.use_gpu = True if torch.cuda.is_available() else False\n",
    "        self.learning_rate = 0.0002\n",
    "        self.beta1 = .5\n",
    "        self.svhn_loader_train = svhn_loader_train\n",
    "        self.svhn_loader_test = svhn_loader_test\n",
    "        self.epochs = 25\n",
    "        self.batch_size = batch_size\n",
    "        self.out_dir = './train_out'\n",
    "\n",
    "        self.netG, self.netD = self._build_model()\n",
    "        self.g_optimizer, self.d_optimizer = self._create_optimizers()\n",
    "\n",
    "    def _build_model(self):\n",
    "        netG = _netG(\n",
    "            self.nz, self.ngf, self.alpha,\n",
    "            self.nc, self.use_gpu)\n",
    "        netG.apply(self._weights_init)\n",
    "        print(netG)\n",
    "        # TODO: load weights from file if it exists\n",
    "\n",
    "        netD = _netD(\n",
    "            self.ndf, self.alpha, self.nc,\n",
    "            self.drop_rate, self.num_classes, self.use_gpu)\n",
    "        netD.apply(self._weights_init)\n",
    "        print(netD)\n",
    "        # TODO: load weights from file if it exists\n",
    "\n",
    "        if self.use_gpu:\n",
    "            netG = netG.cuda()\n",
    "            netD = netD.cuda()\n",
    "\n",
    "        return netG, netD\n",
    "\n",
    "    def _weights_init(self, module):\n",
    "        '''\n",
    "        Custom weights initialization called on netG and netD\n",
    "        '''\n",
    "        classname = module.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            module.weight.data.normal_(0.0, 0.02)\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            module.weight.data.normal_(1.0, 0.02)\n",
    "            module.bias.data.fill_(0)\n",
    "\n",
    "    def _create_optimizers(self):\n",
    "        g_params = list(self.netG.parameters())\n",
    "        d_params = list(self.netD.parameters())\n",
    "\n",
    "        g_optimizer = optim.Adam(g_params, self.learning_rate, betas=(self.beta1, 0.999))\n",
    "        d_optimizer = optim.Adam(d_params, self.learning_rate, betas=(self.beta1, 0.999))\n",
    "\n",
    "        return g_optimizer, d_optimizer\n",
    "\n",
    "    def _to_var(self, x):\n",
    "        if self.use_gpu:\n",
    "            x = x.cuda()\n",
    "        return Variable(x)\n",
    "\n",
    "    def _one_hot(self, x):\n",
    "        label_numpy = x.data.cpu().numpy()\n",
    "        label_onehot = np.zeros((label_numpy.shape[0], self.num_classes))\n",
    "        label_onehot[np.arange(label_numpy.shape[0]), label_numpy] = 1\n",
    "        label_onehot = self._to_var(torch.FloatTensor(label_onehot))\n",
    "        return label_onehot\n",
    "    \n",
    "    def _reset_grad(self):\n",
    "        self.g_optimizer.zero_grad()\n",
    "        self.d_optimizer.zero_grad()\n",
    "\n",
    "    def train(self):\n",
    "        if not os.path.exists(self.out_dir):\n",
    "            os.makedirs(self.out_dir)\n",
    "        \n",
    "        d_gan_criterion = nn.BCEWithLogitsLoss()\n",
    "        d_gan_class_criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        noise = torch.FloatTensor(self.batch_size, self.nz, 1, 1)\n",
    "        \n",
    "        fixed_noise = torch.FloatTensor(self.batch_size, self.nz, 1, 1).normal_(0, 1)\n",
    "        fixed_noise = self._to_var(fixed_noise)\n",
    "        \n",
    "        d_gan_labels_real = torch.LongTensor(batch_size)\n",
    "        d_gan_labels_fake = torch.LongTensor(batch_size)\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            masked_correct = 0\n",
    "            num_samples = 0\n",
    "            \n",
    "            for i, data in enumerate(self.svhn_loader_train):\n",
    "                # load svhn dataset\n",
    "                svhn_data, svhn_labels, label_mask = data\n",
    "                svhn_data = self._to_var(svhn_data)\n",
    "                svhn_labels = self._to_var(svhn_labels).long().squeeze()\n",
    "                label_mask = self._to_var(label_mask).float().squeeze()\n",
    "\n",
    "                # -------------- train netD --------------\n",
    "\n",
    "                self._reset_grad()\n",
    "\n",
    "                # train with real images\n",
    "                # d_out == softmax(d_class_logits)\n",
    "                d_out, d_class_logits_on_data, d_gan_logits_real, d_sample_features = self.netD(svhn_data)\n",
    "                d_gan_labels_real.resize_as_(svhn_labels.data.cpu()).fill_(1)\n",
    "                d_gan_labels_real_var = self._to_var(d_gan_labels_real).float()\n",
    "                d_gan_loss_real = d_gan_criterion(\n",
    "                    d_gan_logits_real,\n",
    "                    d_gan_labels_real_var)\n",
    "                \n",
    "                # train with fake images\n",
    "                noise.resize_(batch_size, self.nz, 1, 1).normal_(0, 1)\n",
    "                noise_var = self._to_var(noise)\n",
    "                fake = self.netG(noise_var)\n",
    "\n",
    "                # call detach() to avoid backprop for netG here\n",
    "                _, _, d_gan_logits_fake, _ = self.netD(fake.detach())\n",
    "                d_gan_labels_fake.resize_(batch_size).fill_(0)\n",
    "                d_gan_labels_fake_var = self._to_var(d_gan_labels_fake).float()\n",
    "                d_gan_loss_fake = d_gan_criterion(\n",
    "                    d_gan_logits_fake,\n",
    "                    d_gan_labels_fake_var)\n",
    "\n",
    "                d_gan_loss = d_gan_loss_real + d_gan_loss_fake\n",
    "\n",
    "                # d_out == softmax(d_class_logits)\n",
    "                # see https://stackoverflow.com/questions/34240703/whats-the-difference-between-softmax-and-softmax-cross-entropy-with-logits/39499486#39499486\n",
    "                svhn_labels_one_hot = self._one_hot(svhn_labels)\n",
    "                d_class_loss_entropy = -torch.sum(svhn_labels_one_hot * torch.log(d_out), dim=1)\n",
    "                \n",
    "                d_class_loss_entropy = d_class_loss_entropy.squeeze()\n",
    "                # delim = torch.max(torch.Tensor([1.0, torch.sum(label_mask.data)]))\n",
    "                # d_class_loss = torch.sum(label_mask * d_class_loss_entropy) / delim\n",
    "                numpy_labels = svhn_labels.data.cpu().numpy()\n",
    "                d_class_loss = torch.sum(d_class_loss_entropy) / numpy_labels.shape[0]\n",
    "                \n",
    "                d_loss = d_gan_loss + d_class_loss\n",
    "                \n",
    "                d_loss.backward()\n",
    "                self.d_optimizer.step()\n",
    "\n",
    "                # -------------- update netG --------------\n",
    "                \n",
    "                self._reset_grad()\n",
    "\n",
    "                # call netD again to do backprop for netG here\n",
    "                noise.resize_(batch_size, self.nz, 1, 1).normal_(0, 1)\n",
    "                noise_var = self._to_var(noise)\n",
    "                fake = self.netG(noise_var)\n",
    "\n",
    "                _, _, _, d_data_features = self.netD(fake)\n",
    "                \n",
    "                # Here we set `g_loss` to the \"feature matching\" loss invented by Tim Salimans at OpenAI.\n",
    "                # This loss consists of minimizing the absolute difference between the expected features\n",
    "                # on the data and the expected features on the generated samples.\n",
    "                # This loss works better for semi-supervised learning than the tradition GAN losses.\n",
    "                data_features_mean = torch.mean(d_data_features, dim=0).squeeze()\n",
    "                sample_features_mean = torch.mean(d_sample_features.detach(), dim=0).squeeze()\n",
    "                \n",
    "                g_loss = torch.mean(torch.abs(data_features_mean - sample_features_mean))\n",
    "\n",
    "                g_loss.backward()\n",
    "                self.g_optimizer.step()\n",
    "\n",
    "                _, pred_class = torch.max(d_class_logits_on_data, 1)\n",
    "                eq = torch.eq(svhn_labels, pred_class)\n",
    "                correct = torch.sum(eq.float())\n",
    "                # masked_correct += torch.sum(label_mask * eq.float())\n",
    "                masked_correct += correct\n",
    "                # num_samples += torch.sum(label_mask)\n",
    "                num_samples += numpy_labels.shape[0]\n",
    "                \n",
    "                if i % 200 == 0:\n",
    "                    print('Training:\\tepoch {}/{}\\tdiscr. gan loss {}\\tdiscr. class loss {}\\tgen loss {}\\tsamples {}/{}'.\n",
    "                        format(epoch, self.epochs, d_gan_loss.data[0], d_class_loss.data[0], g_loss.data[0], \n",
    "                               i + 1, len(self.svhn_loader_train)))\n",
    "                    real_cpu, _, _ = data\n",
    "                    vutils.save_image(real_cpu,\n",
    "                            '%s/real_samples.png' % self.out_dir,\n",
    "                            normalize=True)\n",
    "                    fake = self.netG(fixed_noise)\n",
    "                    vutils.save_image(fake.data,\n",
    "                            '%s/fake_samples_epoch_%03d.png' % (self.out_dir, epoch),\n",
    "                            normalize=True)\n",
    "                    \n",
    "            # accuracy = masked_correct.data[0]/max(1.0, num_samples.data[0])\n",
    "            print('Training:\\tepoch {}/{}\\taccuracy {}/{}'.format(epoch, self.epochs, masked_correct, num_samples))\n",
    "\n",
    "            correct = 0\n",
    "            num_samples = 0\n",
    "            for i, data in enumerate(self.svhn_loader_test):\n",
    "                # load svhn dataset\n",
    "                svhn_data, svhn_labels = data\n",
    "                svhn_data = self._to_var(svhn_data)\n",
    "                svhn_labels = self._to_var(svhn_labels).long().squeeze()\n",
    "\n",
    "                # -------------- train netD --------------\n",
    "\n",
    "                # train with real images\n",
    "                d_out, d_class_logits, _, _ = self.netD(svhn_data)\n",
    "                _, pred_idx = torch.max(d_class_logits.data, 1)\n",
    "                eq = torch.eq(svhn_labels.data, pred_idx)\n",
    "                correct += torch.sum(eq.float())\n",
    "                num_samples += len(svhn_labels)\n",
    "                \n",
    "                if i % 50 == 0:\n",
    "                    print('Test:\\tepoch {}/{}\\tsamples {}/{}'.format(\n",
    "                        epoch, self.epochs, i + 1, len(self.svhn_loader_test)))\n",
    "                \n",
    "            # accuracy = correct/max(1.0, 1.0 * num_samples)\n",
    "            print('Test:\\tepoch {}/{}\\taccuracy {}/{}'.format(epoch, self.epochs, correct, num_samples))\n",
    "\n",
    "            # do checkpointing\n",
    "            torch.save(self.netG.state_dict(), '%s/netG_epoch_%d.pth' % (self.out_dir, epoch))\n",
    "            torch.save(self.netD.state_dict(), '%s/netD_epoch_%d.pth' % (self.out_dir, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_loader_train, svhn_loader_test = get_loader(image_size, batch_size)\n",
    "solver = Solver(svhn_loader_train, svhn_loader_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\tepoch 1/25\tdiscr. gan loss 2.534649133682251\tdiscr. class loss 4.167726516723633\tgen loss 0.0419347807765007\tsamples 1/1145\n",
      "Training:\tepoch 1/25\tdiscr. gan loss 1.090012550354004\tdiscr. class loss 4.240676403045654\tgen loss 0.3187779188156128\tsamples 201/1145\n",
      "Training:\tepoch 1/25\tdiscr. gan loss 1.0721180438995361\tdiscr. class loss 4.199171543121338\tgen loss 0.3505549430847168\tsamples 401/1145\n",
      "Training:\tepoch 1/25\tdiscr. gan loss 1.047226905822754\tdiscr. class loss 3.856647491455078\tgen loss 0.17716744542121887\tsamples 601/1145\n",
      "Training:\tepoch 1/25\tdiscr. gan loss 0.9562832713127136\tdiscr. class loss 3.523123264312744\tgen loss 0.3336448669433594\tsamples 801/1145\n",
      "Training:\tepoch 1/25\tdiscr. gan loss 0.8121185302734375\tdiscr. class loss 3.1991424560546875\tgen loss 0.17021872103214264\tsamples 1001/1145\n",
      "Training:\tepoch 1/25\taccuracy Variable containing:\n",
      " 24270\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 1/25\tsamples 1/407\n",
      "Test:\tepoch 1/25\tsamples 51/407\n",
      "Test:\tepoch 1/25\tsamples 101/407\n",
      "Test:\tepoch 1/25\tsamples 151/407\n",
      "Test:\tepoch 1/25\tsamples 201/407\n",
      "Test:\tepoch 1/25\tsamples 251/407\n",
      "Test:\tepoch 1/25\tsamples 301/407\n",
      "Test:\tepoch 1/25\tsamples 351/407\n",
      "Test:\tepoch 1/25\tsamples 401/407\n",
      "Test:\tepoch 1/25\taccuracy 13858.0/26032\n",
      "Training:\tepoch 2/25\tdiscr. gan loss 0.9166621565818787\tdiscr. class loss 3.215245008468628\tgen loss 0.135205939412117\tsamples 1/1145\n",
      "Training:\tepoch 2/25\tdiscr. gan loss 0.6129348278045654\tdiscr. class loss 3.376582145690918\tgen loss 0.6980297565460205\tsamples 201/1145\n",
      "Training:\tepoch 2/25\tdiscr. gan loss 0.7781004309654236\tdiscr. class loss 3.0701072216033936\tgen loss 0.10597815364599228\tsamples 401/1145\n",
      "Training:\tepoch 2/25\tdiscr. gan loss 0.9914363026618958\tdiscr. class loss 3.19842791557312\tgen loss 0.09420008212327957\tsamples 601/1145\n",
      "Training:\tepoch 2/25\tdiscr. gan loss 1.1493512392044067\tdiscr. class loss 2.9821548461914062\tgen loss 0.33026444911956787\tsamples 801/1145\n",
      "Training:\tepoch 2/25\tdiscr. gan loss 0.7741193175315857\tdiscr. class loss 2.8803749084472656\tgen loss 0.6555208563804626\tsamples 1001/1145\n",
      "Training:\tepoch 2/25\taccuracy Variable containing:\n",
      " 37929\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 2/25\tsamples 1/407\n",
      "Test:\tepoch 2/25\tsamples 51/407\n",
      "Test:\tepoch 2/25\tsamples 101/407\n",
      "Test:\tepoch 2/25\tsamples 151/407\n",
      "Test:\tepoch 2/25\tsamples 201/407\n",
      "Test:\tepoch 2/25\tsamples 251/407\n",
      "Test:\tepoch 2/25\tsamples 301/407\n",
      "Test:\tepoch 2/25\tsamples 351/407\n",
      "Test:\tepoch 2/25\tsamples 401/407\n",
      "Test:\tepoch 2/25\taccuracy 14498.0/26032\n",
      "Training:\tepoch 3/25\tdiscr. gan loss 0.4206593632698059\tdiscr. class loss 2.9683480262756348\tgen loss 0.37890198826789856\tsamples 1/1145\n",
      "Training:\tepoch 3/25\tdiscr. gan loss 0.36334463953971863\tdiscr. class loss 2.9863851070404053\tgen loss 0.3908967971801758\tsamples 201/1145\n",
      "Training:\tepoch 3/25\tdiscr. gan loss 0.4750155806541443\tdiscr. class loss 3.1049070358276367\tgen loss 0.35053473711013794\tsamples 401/1145\n",
      "Training:\tepoch 3/25\tdiscr. gan loss 0.5771074891090393\tdiscr. class loss 2.9230706691741943\tgen loss 0.4376014471054077\tsamples 601/1145\n",
      "Training:\tepoch 3/25\tdiscr. gan loss 0.754618227481842\tdiscr. class loss 2.8765134811401367\tgen loss 0.5722296237945557\tsamples 801/1145\n",
      "Training:\tepoch 3/25\tdiscr. gan loss 1.095495581626892\tdiscr. class loss 2.7721915245056152\tgen loss 0.717000424861908\tsamples 1001/1145\n",
      "Training:\tepoch 3/25\taccuracy Variable containing:\n",
      " 39495\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 3/25\tsamples 1/407\n",
      "Test:\tepoch 3/25\tsamples 51/407\n",
      "Test:\tepoch 3/25\tsamples 101/407\n",
      "Test:\tepoch 3/25\tsamples 151/407\n",
      "Test:\tepoch 3/25\tsamples 201/407\n",
      "Test:\tepoch 3/25\tsamples 251/407\n",
      "Test:\tepoch 3/25\tsamples 301/407\n",
      "Test:\tepoch 3/25\tsamples 351/407\n",
      "Test:\tepoch 3/25\tsamples 401/407\n",
      "Test:\tepoch 3/25\taccuracy 14479.0/26032\n",
      "Training:\tepoch 4/25\tdiscr. gan loss 0.43289679288864136\tdiscr. class loss 2.7916982173919678\tgen loss 0.4179261028766632\tsamples 1/1145\n",
      "Training:\tepoch 4/25\tdiscr. gan loss 0.4548509120941162\tdiscr. class loss 2.874859094619751\tgen loss 0.21776294708251953\tsamples 201/1145\n",
      "Training:\tepoch 4/25\tdiscr. gan loss 0.4912303388118744\tdiscr. class loss 2.7624855041503906\tgen loss 0.2955969572067261\tsamples 401/1145\n",
      "Training:\tepoch 4/25\tdiscr. gan loss 0.6346048712730408\tdiscr. class loss 3.1007936000823975\tgen loss 0.24547144770622253\tsamples 601/1145\n",
      "Training:\tepoch 4/25\tdiscr. gan loss 1.4677461385726929\tdiscr. class loss 2.6835906505584717\tgen loss 0.32802990078926086\tsamples 801/1145\n",
      "Training:\tepoch 4/25\tdiscr. gan loss 0.47692570090293884\tdiscr. class loss 2.826691150665283\tgen loss 0.37355878949165344\tsamples 1001/1145\n",
      "Training:\tepoch 4/25\taccuracy Variable containing:\n",
      " 40657\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 4/25\tsamples 1/407\n",
      "Test:\tepoch 4/25\tsamples 51/407\n",
      "Test:\tepoch 4/25\tsamples 101/407\n",
      "Test:\tepoch 4/25\tsamples 151/407\n",
      "Test:\tepoch 4/25\tsamples 201/407\n",
      "Test:\tepoch 4/25\tsamples 251/407\n",
      "Test:\tepoch 4/25\tsamples 301/407\n",
      "Test:\tepoch 4/25\tsamples 351/407\n",
      "Test:\tepoch 4/25\tsamples 401/407\n",
      "Test:\tepoch 4/25\taccuracy 14452.0/26032\n",
      "Training:\tepoch 5/25\tdiscr. gan loss 0.16545575857162476\tdiscr. class loss 2.7377548217773438\tgen loss 0.35889381170272827\tsamples 1/1145\n",
      "Training:\tepoch 5/25\tdiscr. gan loss 0.8749377727508545\tdiscr. class loss 3.1234254837036133\tgen loss 0.08759474009275436\tsamples 201/1145\n",
      "Training:\tepoch 5/25\tdiscr. gan loss 0.29973602294921875\tdiscr. class loss 2.7088427543640137\tgen loss 0.4336828887462616\tsamples 401/1145\n",
      "Training:\tepoch 5/25\tdiscr. gan loss 0.8036664128303528\tdiscr. class loss 2.9090378284454346\tgen loss 0.8122682571411133\tsamples 601/1145\n",
      "Training:\tepoch 5/25\tdiscr. gan loss 0.2912576198577881\tdiscr. class loss 2.5936965942382812\tgen loss 0.6177526712417603\tsamples 801/1145\n",
      "Training:\tepoch 5/25\tdiscr. gan loss 0.5297597050666809\tdiscr. class loss 2.769310474395752\tgen loss 0.2817559540271759\tsamples 1001/1145\n",
      "Training:\tepoch 5/25\taccuracy Variable containing:\n",
      " 41286\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 5/25\tsamples 1/407\n",
      "Test:\tepoch 5/25\tsamples 51/407\n",
      "Test:\tepoch 5/25\tsamples 101/407\n",
      "Test:\tepoch 5/25\tsamples 151/407\n",
      "Test:\tepoch 5/25\tsamples 201/407\n",
      "Test:\tepoch 5/25\tsamples 251/407\n",
      "Test:\tepoch 5/25\tsamples 301/407\n",
      "Test:\tepoch 5/25\tsamples 351/407\n",
      "Test:\tepoch 5/25\tsamples 401/407\n",
      "Test:\tepoch 5/25\taccuracy 14357.0/26032\n",
      "Training:\tepoch 6/25\tdiscr. gan loss 0.4973733723163605\tdiscr. class loss 2.8491153717041016\tgen loss 0.58628249168396\tsamples 1/1145\n",
      "Training:\tepoch 6/25\tdiscr. gan loss 0.3226873278617859\tdiscr. class loss 2.801607370376587\tgen loss 0.2852949798107147\tsamples 201/1145\n",
      "Training:\tepoch 6/25\tdiscr. gan loss 0.6605269312858582\tdiscr. class loss 2.671030282974243\tgen loss 0.37044695019721985\tsamples 401/1145\n",
      "Training:\tepoch 6/25\tdiscr. gan loss 0.32443031668663025\tdiscr. class loss 2.872826337814331\tgen loss 0.549005389213562\tsamples 601/1145\n",
      "Training:\tepoch 6/25\tdiscr. gan loss 0.7841317057609558\tdiscr. class loss 2.8771939277648926\tgen loss 0.08923693001270294\tsamples 801/1145\n",
      "Training:\tepoch 6/25\tdiscr. gan loss 0.15340501070022583\tdiscr. class loss 2.781546115875244\tgen loss 0.28181585669517517\tsamples 1001/1145\n",
      "Training:\tepoch 6/25\taccuracy Variable containing:\n",
      " 42080\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 6/25\tsamples 1/407\n",
      "Test:\tepoch 6/25\tsamples 51/407\n",
      "Test:\tepoch 6/25\tsamples 101/407\n",
      "Test:\tepoch 6/25\tsamples 151/407\n",
      "Test:\tepoch 6/25\tsamples 201/407\n",
      "Test:\tepoch 6/25\tsamples 251/407\n",
      "Test:\tepoch 6/25\tsamples 301/407\n",
      "Test:\tepoch 6/25\tsamples 351/407\n",
      "Test:\tepoch 6/25\tsamples 401/407\n",
      "Test:\tepoch 6/25\taccuracy 16555.0/26032\n",
      "Training:\tepoch 7/25\tdiscr. gan loss 1.0678541660308838\tdiscr. class loss 2.726823329925537\tgen loss 0.11906439810991287\tsamples 1/1145\n",
      "Training:\tepoch 7/25\tdiscr. gan loss 0.35275742411613464\tdiscr. class loss 2.8662047386169434\tgen loss 0.44140899181365967\tsamples 201/1145\n",
      "Training:\tepoch 7/25\tdiscr. gan loss 0.32764700055122375\tdiscr. class loss 2.705456018447876\tgen loss 0.194938063621521\tsamples 401/1145\n",
      "Training:\tepoch 7/25\tdiscr. gan loss 0.2813687026500702\tdiscr. class loss 2.839468240737915\tgen loss 0.4444650411605835\tsamples 601/1145\n",
      "Training:\tepoch 7/25\tdiscr. gan loss 1.615439772605896\tdiscr. class loss 2.8367741107940674\tgen loss 0.08446356654167175\tsamples 801/1145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\tepoch 7/25\tdiscr. gan loss 0.2913511097431183\tdiscr. class loss 3.0248942375183105\tgen loss 0.3762364089488983\tsamples 1001/1145\n",
      "Training:\tepoch 7/25\taccuracy Variable containing:\n",
      " 42309\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 7/25\tsamples 1/407\n",
      "Test:\tepoch 7/25\tsamples 51/407\n",
      "Test:\tepoch 7/25\tsamples 101/407\n",
      "Test:\tepoch 7/25\tsamples 151/407\n",
      "Test:\tepoch 7/25\tsamples 201/407\n",
      "Test:\tepoch 7/25\tsamples 251/407\n",
      "Test:\tepoch 7/25\tsamples 301/407\n",
      "Test:\tepoch 7/25\tsamples 351/407\n",
      "Test:\tepoch 7/25\tsamples 401/407\n",
      "Test:\tepoch 7/25\taccuracy 16266.0/26032\n",
      "Training:\tepoch 8/25\tdiscr. gan loss 0.4859289526939392\tdiscr. class loss 2.816208839416504\tgen loss 0.31552788615226746\tsamples 1/1145\n",
      "Training:\tepoch 8/25\tdiscr. gan loss 0.4358625113964081\tdiscr. class loss 2.603207588195801\tgen loss 0.49591678380966187\tsamples 201/1145\n",
      "Training:\tepoch 8/25\tdiscr. gan loss 0.39033907651901245\tdiscr. class loss 2.712826728820801\tgen loss 0.22160223126411438\tsamples 401/1145\n",
      "Training:\tepoch 8/25\tdiscr. gan loss 0.11236868798732758\tdiscr. class loss 2.8846023082733154\tgen loss 0.39767318964004517\tsamples 601/1145\n",
      "Training:\tepoch 8/25\tdiscr. gan loss 0.2612162232398987\tdiscr. class loss 2.9775729179382324\tgen loss 0.2395136058330536\tsamples 801/1145\n",
      "Training:\tepoch 8/25\tdiscr. gan loss 0.42267167568206787\tdiscr. class loss 2.746339797973633\tgen loss 0.4954233765602112\tsamples 1001/1145\n",
      "Training:\tepoch 8/25\taccuracy Variable containing:\n",
      " 43283\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 8/25\tsamples 1/407\n",
      "Test:\tepoch 8/25\tsamples 51/407\n",
      "Test:\tepoch 8/25\tsamples 101/407\n",
      "Test:\tepoch 8/25\tsamples 151/407\n",
      "Test:\tepoch 8/25\tsamples 201/407\n",
      "Test:\tepoch 8/25\tsamples 251/407\n",
      "Test:\tepoch 8/25\tsamples 301/407\n",
      "Test:\tepoch 8/25\tsamples 351/407\n",
      "Test:\tepoch 8/25\tsamples 401/407\n",
      "Test:\tepoch 8/25\taccuracy 14296.0/26032\n",
      "Training:\tepoch 9/25\tdiscr. gan loss 0.39282241463661194\tdiscr. class loss 2.596266984939575\tgen loss 0.5691854953765869\tsamples 1/1145\n",
      "Training:\tepoch 9/25\tdiscr. gan loss 1.1886688470840454\tdiscr. class loss 2.8968660831451416\tgen loss 0.7173036336898804\tsamples 201/1145\n",
      "Training:\tepoch 9/25\tdiscr. gan loss 0.43896713852882385\tdiscr. class loss 2.4153711795806885\tgen loss 0.5288152098655701\tsamples 401/1145\n",
      "Training:\tepoch 9/25\tdiscr. gan loss 0.834312915802002\tdiscr. class loss 2.9117112159729004\tgen loss 0.7802323698997498\tsamples 601/1145\n",
      "Training:\tepoch 9/25\tdiscr. gan loss 0.10875223577022552\tdiscr. class loss 2.437587022781372\tgen loss 0.6485617160797119\tsamples 801/1145\n",
      "Training:\tepoch 9/25\tdiscr. gan loss 0.3913598954677582\tdiscr. class loss 2.5275001525878906\tgen loss 0.5308423042297363\tsamples 1001/1145\n",
      "Training:\tepoch 9/25\taccuracy Variable containing:\n",
      " 43300\n",
      "[torch.cuda.FloatTensor of size 1 (GPU 0)]\n",
      "/73257\n",
      "Test:\tepoch 9/25\tsamples 1/407\n",
      "Test:\tepoch 9/25\tsamples 51/407\n",
      "Test:\tepoch 9/25\tsamples 101/407\n",
      "Test:\tepoch 9/25\tsamples 151/407\n",
      "Test:\tepoch 9/25\tsamples 201/407\n",
      "Test:\tepoch 9/25\tsamples 251/407\n",
      "Test:\tepoch 9/25\tsamples 301/407\n",
      "Test:\tepoch 9/25\tsamples 351/407\n",
      "Test:\tepoch 9/25\tsamples 401/407\n",
      "Test:\tepoch 9/25\taccuracy 16256.0/26032\n",
      "Training:\tepoch 10/25\tdiscr. gan loss 0.19309943914413452\tdiscr. class loss 2.813720703125\tgen loss 0.3600348234176636\tsamples 1/1145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-39:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ad90b845b08b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-281dfe363eba>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_class_logits_on_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0meq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvhn_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m                 \u001b[0;31m# masked_correct += torch.sum(label_mask * eq.float())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mmasked_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
